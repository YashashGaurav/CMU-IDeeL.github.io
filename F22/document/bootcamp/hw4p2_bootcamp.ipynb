{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW4P2_template_F21.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HTGPr98x0yjO",
        "outputId": "de40fb04-6b61-4d7e-cf10-e992421129fc"
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "import torch.optim as optim\n",
        "import torch.nn.utils as utils\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "from torch.utils import data\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "print(cuda, sys.version)\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "np.random.seed(420)\n",
        "torch.manual_seed(420)\n",
        "\n",
        "LETTER_LIST = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \n",
        "               'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '-', \"'\", '.', '_', '+', ' ', '<eos>']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-5-f795a9bde53f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ZFmcvE1pkDNi",
        "outputId": "61152a6a-8e4b-4725-868e-7218465fc454"
      },
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def create_dictionaries(letter_list: List[str]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
        "    '''\n",
        "    Create dictionaries for letter2index and index2letter transformations\n",
        "    '''\n",
        "    pass\n",
        "    \n",
        "def transform_letter_to_index(raw_transcripts: List[str]) -> List[int]:\n",
        "    '''\n",
        "    Transforms text input to numerical input by converting each letter \n",
        "    to its corresponding index from letter_list\n",
        "\n",
        "    Args:\n",
        "        raw_transcripts: Raw text transcripts with the shape of (N, )\n",
        "    \n",
        "    Return:\n",
        "        transcripts: Converted index-format transcripts. This would be a list with a length of N\n",
        "    '''  \n",
        "    pass\n",
        "    \n",
        "# Create the letter2index and index2letter dictionary\n",
        "letter2index, index2letter = create_dictionaries(LETTER_LIST)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LETTER_LIST' is not defined",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-1-e57ced53f82b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Create the letter2index and index2letter dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mletter2index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex2letter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_dictionaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLETTER_LIST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m: name 'LETTER_LIST' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJukRb802lQI"
      },
      "source": [
        "# Load the training, validation and testing data\n",
        "train_data = np.load('train.npy', allow_pickle=True, encoding='bytes')\n",
        "valid_data = np.load('dev.npy', allow_pickle=True, encoding='bytes')\n",
        "test_data = np.load('test.npy', allow_pickle=True, encoding='bytes')\n",
        "\n",
        "# Load the training, validation raw text transcripts\n",
        "raw_train_transcript = np.load('train_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "raw_valid_transcript = np.load('dev_transcripts.npy', allow_pickle=True,encoding='bytes')\n",
        "\n",
        "# TODO: Convert the raw text transcripts into indexes\n",
        "train_transcript = # fill this out\n",
        "valid_transcript = # fill this out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhSD0FOXm5Q6"
      },
      "source": [
        "class MyDataset(data.Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X#.astype() if you want its data type converted\n",
        "        self.Y = Y#.astype()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # For testing set, return only x\n",
        "        if self.Y is None:\n",
        "            return torch.as_tensor(self.X[index])\n",
        "        # For training and validation set, return x and y\n",
        "        else:\n",
        "            return torch.as_tensor(self.X[index]), torch.as_tensor(self.Y[index])\n",
        "\n",
        "def collate_train_val(data):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "        pad_x: the padded x (training/validation speech data) \n",
        "        pad_y: the padded y (text labels - transcripts)\n",
        "        x_len: the length of x\n",
        "        y_len: the length of y\n",
        "    \"\"\"\n",
        "    # Consider batch_first = True\n",
        "    pass\n",
        "\n",
        "def collate_test(data):\n",
        "    \"\"\"\n",
        "    Return:\n",
        "        pad_x: the padded x (testing speech data) \n",
        "        x_len: the length of x\n",
        "    \"\"\"\n",
        "    # Consider batch_first = True\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "collapsed": true,
        "id": "OJKdna5VnIJM",
        "outputId": "619dba22-f9ef-4bd0-a4f3-964f94037df8"
      },
      "source": [
        "# Create datasets\n",
        "train_dataset = # fill this out\n",
        "valid_dataset = # fill this out\n",
        "test_dataset = # fill this out\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = # fill this out\n",
        "valid_loader = # fill this out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-2951b5189539>, line 2)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-2951b5189539>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    train_dataset =\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfpIMUDzCvT3"
      },
      "source": [
        "class pBLSTM(nn.Module):\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed\n",
        "    2. Truncate the input length dimension by concatenating feature dimension\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(# fill this out, num_layers=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F9zAQR95P55"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    '''\n",
        "    Encoder takes the utterances as inputs and returns the key, value and unpacked_x_len.\n",
        "\n",
        "    '''\n",
        "    def __init__(self, input_dim, encoder_hidden_dim, key_value_size=128):\n",
        "        super(Encoder, self).__init__()\n",
        "        # The first LSTM layer at the bottom\n",
        "        self.lstm = nn.LSTM(# fill this out)\n",
        "\n",
        "        # Define the blocks of pBLSTMs\n",
        "        # Dimensions should be chosen carefully\n",
        "        # Hint: Bidirectionality, truncation...\n",
        "        self.pBLSTMs = nn.Sequential(\n",
        "            pBLSTM(# fill this out),\n",
        "            # Optional: dropout\n",
        "            # ...\n",
        "        )\n",
        "         \n",
        "        # Optional: The linear transformations for producing Key and Value for attention\n",
        "        # Hint: Dimensions when bidirectional lstm? \n",
        "        # self.key_network = nn.Linear()\n",
        "        # self.value_network = nn.Linear()\n",
        "\n",
        "    def forward(self, x, x_len):\n",
        "        \"\"\"\n",
        "        1. Pack your input and pass it through the first LSTM layer (no truncation)\n",
        "        2. Pass it through the pyramidal LSTM layer\n",
        "        3. Pad your input back to (B, T, *) or (T, B, *) shape\n",
        "        4. Output Key, Value, and truncated input lens\n",
        "\n",
        "        Key and value could be\n",
        "            (i) Concatenated hidden vectors from all time steps (key == value).\n",
        "            (ii) Linear projections of the output from the last pBLSTM network.\n",
        "                If you choose this way, you can use the final output of\n",
        "                your pBLSTM network.\n",
        "        \"\"\"\n",
        "\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjZ0N7t5WKp5"
      },
      "source": [
        "encoder = # fill this out\n",
        "# Try out your encoder on a tiny input before moving to the next step...\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqu-MUM8TjUO"
      },
      "source": [
        "def plot_attention(attention):\n",
        "    # utility function for debugging\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    '''\n",
        "    Attention is calculated using key and value from encoder and query from decoder.\n",
        "    Here are different ways to compute attention and context:\n",
        "    1. Dot-product attention\n",
        "        energy = bmm(key, query) \n",
        "        # Optional: Scaled dot-product by normalizing with sqrt key dimension\n",
        "        # Check \"attention is all you need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore...\n",
        "    2. Cosine attention\n",
        "        energy = cosine(query, key) # almost the same as dot-product xD \n",
        "    3. Bi-linear attention\n",
        "        W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "        energy = bmm(key @ W, query)\n",
        "    4. Multi-layer perceptron\n",
        "        # Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "    \n",
        "    After obtaining unnormalized attention weights (energy), compute and return attention and context, i.e.,\n",
        "    energy = mask(energy) # mask out padded elements with big negative number (e.g. -1e9)\n",
        "    attention = softmax(energy)\n",
        "    context = bmm(attention, value)\n",
        "\n",
        "    5. Multi-Head Attention\n",
        "        # Check \"attention is all you need\" Section 3.2.2\n",
        "        h = Number of heads\n",
        "        W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "        W_O: d_v -> d_v\n",
        "\n",
        "        Reshape K: (B, T, d_k)\n",
        "        to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "        Reshape V: (B, T, d_v)\n",
        "        to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "        Reshape Q: (B, d_q)\n",
        "        to (B, h, d_q // h)\n",
        "\n",
        "        energy = Q @ K^T\n",
        "        energy = mask(energy)\n",
        "        attention = softmax(energy)\n",
        "        multi_head = attention @ V\n",
        "        multi_head = multi_head reshaped to (B, d_v)\n",
        "        context = multi_head @ W_O\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "        # Optional: dropout\n",
        "\n",
        "    def forward(self, query, key, value, mask):\n",
        "        \"\"\"\n",
        "        input:\n",
        "            key: (batch_size, seq_len, d_k)\n",
        "            value: (batch_size, seq_len, d_v)\n",
        "            query: (batch_size, d_q)\n",
        "        * Hint: d_k == d_v == d_q is often true if you use linear projections\n",
        "        return:\n",
        "            context: (batch_size, key_val_dim)\n",
        "        \n",
        "        \"\"\"\n",
        "        pass\n",
        "        # return context, attention\n",
        "        # we return attention weights for plotting (for debugging)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcTC4cK95TYT"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    '''\n",
        "    As mentioned in a previous recitation, each forward call of decoder deals with just one time step.\n",
        "    Thus we use LSTMCell instead of LSTM here.\n",
        "    The output from the last LSTMCell can be used as a query for calculating attention.\n",
        "    Methods like Gumble noise and teacher forcing can also be incorporated for improving the performance.\n",
        "    '''\n",
        "    def __init__(self, vocab_size, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Hint: Be careful with the padding_idx\n",
        "        self.embedding = nn.Embedding(# fill this out)\n",
        "        # The number of cells is defined based on the paper\n",
        "        self.lstm1 = nn.LSTMCell(# fill this out)\n",
        "        self.lstm2 = nn.LSTMCell(# fill this out)\n",
        "    \n",
        "        self.attention = Attention()     \n",
        "        self.vocab_size = vocab_size\n",
        "        # Optional: Weight-tying\n",
        "        self.character_prob = nn.Linear(# fill this out) #: d_v -> vocab_size\n",
        "        self.key_value_size = key_value_size\n",
        "        \n",
        "        # Weight tying\n",
        "        self.character_prob.weight = self.embedding.weight\n",
        "\n",
        "    def forward(self, key, value, encoder_len, y=None, mode='train'):\n",
        "        '''\n",
        "        Args:\n",
        "            key :(B, T, d_k) - Output of the Encoder (possibly from the Key projection layer)\n",
        "            value: (B, T, d_v) - Output of the Encoder (possibly from the Value projection layer)\n",
        "            y: (B, text_len) - Batch input of text with text_length\n",
        "            mode: Train or eval mode for teacher forcing\n",
        "        Return:\n",
        "            predictions: the character perdiction probability \n",
        "        '''\n",
        "\n",
        "        B, key_seq_max_len, key_value_size = key.shape\n",
        "\n",
        "        if mode == 'train':\n",
        "            max_len =  y.shape[1]\n",
        "            char_embeddings = self.embedding(# fill this out)\n",
        "        else:\n",
        "            max_len = 600\n",
        "\n",
        "        # TODO: Create the attention mask here (outside the for loop rather than inside) to aviod repetition\n",
        "        mask = # fill this out\n",
        "        mask = mask.to(device)\n",
        "        \n",
        "        predictions = []\n",
        "        # This is the first input to the decoder\n",
        "        # What should the fill_value be?\n",
        "        prediction = torch.full((batch_size,), fill_value=# fill this out, device=device)\n",
        "        # The length of hidden_states vector should depend on the number of LSTM Cells defined in init\n",
        "        # The paper uses 2\n",
        "        hidden_states = [None, None] \n",
        "        \n",
        "        # TODO: Initialize the context\n",
        "        context = # fill this out\n",
        "\n",
        "        attention_plot = [] # this is for debugging\n",
        "\n",
        "        for i in range(max_len):\n",
        "            if mode == 'train':\n",
        "                # TODO: Implement Teacher Forcing\n",
        "                \"\"\"\n",
        "                if using teacher_forcing:\n",
        "                    if i == 0:\n",
        "                        # This is the first time step\n",
        "                        # Hint: How did you initialize \"prediction\" variable above?\n",
        "                    else:\n",
        "                        # Otherwise, feed the label of the **previous** time step\n",
        "                else:\n",
        "                    char_embed = embedding of the previous prediction\n",
        "                \"\"\"     \n",
        "            else:\n",
        "                char_embed = # embedding of the previous prediction\n",
        "\n",
        "            # what vectors should be concatenated as a context?\n",
        "            y_context = torch.cat([# fill this out], dim=1)\n",
        "            # context and hidden states of lstm 1 from the previous time step should be fed\n",
        "            hidden_states[0] = self.lstm1(# fill this out)\n",
        "\n",
        "            # hidden states of lstm1 and hidden states of lstm2 from the previous time step should be fed\n",
        "            hidden_states[1] = self.lstm2(# fill this out)\n",
        "            # What then is the query?\n",
        "            query = # fill this out\n",
        "            \n",
        "            # Compute attention from the output of the second LSTM Cell\n",
        "            context, attention = self.attention(query, key, value, mask)\n",
        "            # We store the first attention of this batch for debugging\n",
        "            attention_plot.append(attention[0].detach().cpu())\n",
        "            \n",
        "            # What should be concatenated as the output context?\n",
        "            output_context = torch.cat([# fill this out], dim=1)\n",
        "            prediction = self.character_prob(output_context)\n",
        "            # store predictions\n",
        "            predictions.append(# fill this out)\n",
        "        \n",
        "        # Concatenate the attention and predictions to return\n",
        "        attentions = torch.stack(attention_plot, dim=0)\n",
        "        predictions = torch.cat(predictions, dim=1)\n",
        "        return predictions, attentions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d35FEZhz5Uhx"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    '''\n",
        "    We train an end-to-end sequence to sequence model comprising of Encoder and Decoder.\n",
        "    This is simply a wrapper \"model\" for your encoder and decoder.\n",
        "    '''\n",
        "    def __init__(self, input_dim, vocab_size, encoder_hidden_dim, decoder_hidden_dim, embed_dim, key_value_size=128):\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.encoder = Encoder(# fill this out)\n",
        "        self.decoder = Decoder(# fill this out)\n",
        "\n",
        "    def forward(self, x, x_len, y=None, mode='train'):\n",
        "        key, value, encoder_len = self.encoder(x, x_len)\n",
        "        predictions = self.decoder(key, value, encoder_len, y=y, mode=mode)\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jzpCjd9R5VYV",
        "outputId": "039cedd6-626f-4db7-e394-82ddde2059af"
      },
      "source": [
        "def train(model, train_loader, criterion, optimizer, mode):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    \n",
        "    \n",
        "    for i, batch in enumerate(trian_loader):\n",
        "        # x, x_len, y = batch\n",
        "        predictions, attentions = model(x, x_len, y, mode=mode)\n",
        "        \n",
        "        # Generate a mask based on target length. This is to mark padded elements\n",
        "        # so that we can exclude them from computing loss\n",
        "        mask = # fill this out\n",
        "            \n",
        "        # Make sure you have the correct shape of predictions when putting into criterion\n",
        "        loss = criterion(# fill this out)\n",
        "        # Use the mask you defined above to compute the average loss\n",
        "        masked_loss = # fill this out\n",
        "\n",
        "        # backprop\n",
        "        \n",
        "        # Optional: Gradient clipping\n",
        "\n",
        "        # When computing Levenshtein distance, make sure you truncate prediction/target\n",
        "\n",
        "        # Optional: plot your attention for debugging\n",
        "        # plot_attention(attentions)\n",
        "        \n",
        "def val(model, valid_loader):\n",
        "    model.eval()\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-3-143b1ac22a6b>, line 23)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-143b1ac22a6b>\"\u001b[1;36m, line \u001b[1;32m23\u001b[0m\n\u001b[1;33m    10) Use torch.nn.utils.clip_grad_norm(model.parameters(), 2)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PtnTI-s8q4ls",
        "outputId": "acd53df2-b69c-452e-ec59-5b26e552b5c9"
      },
      "source": [
        "# TODO: Define your model and put it on the device here\n",
        "# ...\n",
        "\n",
        "n_epochs = 10\n",
        "optimizer = optim.Adam(model.parameters(), # fill this out)\n",
        "# Make sure you understand the implication of setting reduction = 'none'\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')\n",
        "mode = 'train'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train(model, train_loader, criterion, optimizer, mode)\n",
        "    val(model, valid_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-4-eaa634bc5b73>, line 5)",
          "traceback": [
            "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-eaa634bc5b73>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    optimizer = optim.Adam(model.parameters(), lr=0.001)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mf4fo0FGWKp9"
      },
      "source": [
        "\"\"\"\n",
        "Eason's debugging suggestions:\n",
        "\n",
        "(1) Decrease your batch_size to 2 and print out the value and shape of all intermediate variables to check if they satisfy the expectation\n",
        "(2) Be super careful about the LR, don't make it too high. Too large LR would lead to divergence and your attention plot will never make sense\n",
        "(3) Make sure you have correctly handled the situation for time_step = 0 when teacher forcing\n",
        "\n",
        "(1) is super important and is the most efficient way for debugging. \n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "Eason's tips for passing A from B (from easy to hard):\n",
        "** You need to implement all of these yourself without utilizing any library **\n",
        "(1) Increase model capacity. E.g. increase num_layer of lstm\n",
        "(2) LR and Teacher Forcing are also very important, you can tune them or their scheduler as well. Do NOT change lr or tf during the warm-up stage!\n",
        "(3) Weight tying\n",
        "(4) Locked Dropout - insert between the plstm layers\n",
        "(5) Pre-training decoder or train an LM to help make predictions\n",
        "(5) Pre-training decoder to speed up the convergence: \n",
        "    disable your encoder and only train the decoder like train a language model\n",
        "(6) Better weight initialization technique\n",
        "(7) Batch Norm between plstm. You definitely can try other positions as well\n",
        "(8) Data Augmentation. Time-masking, frequency masking\n",
        "(9) Weight smoothing (avg the last few epoch's weight)\n",
        "(10) You can try CNN + Maxpooling (Avg). Some students replace the entire plstm blocks with it and some just combine them together.\n",
        "(11) Beam Search\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}