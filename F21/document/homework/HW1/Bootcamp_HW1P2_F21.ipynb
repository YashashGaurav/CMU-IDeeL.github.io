{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.7 64-bit ('base': conda)"
    },
    "interpreter": {
      "hash": "627c243d0bd314a2fea4506ebd8ca522b68dafb0e7468df7979a81b3dbd048c4"
    },
    "colab": {
      "name": "Bootcamp_HW1P2_F21_Draft.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46sV04HFTnFi"
      },
      "source": [
        "- Section with imports + notes on what they are\n",
        "- Section on loading data w/reference back to dataloaders\n",
        "- Include note on importing from kaggle\n",
        "- Model + dataset section w/extra notes\n",
        "    - talk about context in dataset section \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzx6BrmZTnFn"
      },
      "source": [
        "# Importing Libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SLfMW6cTnFo"
      },
      "source": [
        "#These libraries help to interact with the operating system and the runtime environment respectively\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#Model/Training related libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "#Dataloader libraries\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZJe5-hlTnFr"
      },
      "source": [
        "# Setting up Data\n",
        "\n",
        "To be able to use our data in training, the data must be loaded and stored in a workable form. In our case, we want to reformat our data into a pytorch tensor - which we can then store into a Dataset and Dataset Loader that can be used by our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2CFUM-PTnFu"
      },
      "source": [
        "### 1. Load Data\n",
        "\n",
        "\n",
        "\n",
        "All homework datasets are accessible through Kaggle. Ideally, you don't want to download all the data locally onto your laptop. Instead, they can be loaded to the temporary process your notebook is working on. We will show an example here, with some tips on dataformatting. For more details, reference the Colab recitation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Hlvgup_wYv"
      },
      "source": [
        "#Intall Kaggle API and create kaggle directory\n",
        "!pip install kaggle\n",
        "!mkdir .kaggle\n",
        "#This data is used to login  into your Kaggle account\n",
        "import json\n",
        "token = {\"username\":\"username\",\"key\":\"key\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5Cf5ClPJl2I"
      },
      "source": [
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "!kaggle config set -n path -v /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSLSs0MUjVRE"
      },
      "source": [
        "#Download dataset .npz files from kaggle\n",
        "!kaggle competitions download -c idl-fall2021-hw1p2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFFS-N3lEnPz"
      },
      "source": [
        "!unzip competitions/idl-fall2021-hw1p2/train.npy.zip\n",
        "!unzip competitions/idl-fall2021-hw1p2/train_labels.npy.zip\n",
        "!unzip competitions/idl-fall2021-hw1p2/dev.npy.zip\n",
        "!unzip competitions/idl-fall2021-hw1p2/dev_labels.npy.zip\n",
        "!unzip competitions/idl-fall2021-hw1p2/test.npy.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_W7QsdFTnFt"
      },
      "source": [
        "### 2. Set up Dataset Class\n",
        "\n",
        "The dataset class is used to format the input/output pairs and store them as well. For more details, recall the Dataset and Dataloader recitation on how each of these features work, as well as the OOP lecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4qbPfrXG9li"
      },
      "source": [
        "class MLPDataset(Dataset):\n",
        "  def __init__(self, data, labels, context=0):\n",
        "    self.data = np.concatenate(np.load(data,allow_pickle=True),axis=0)\n",
        "    self.labels = np.concatenate(np.load(labels,allow_pickle=True),axis=0)\n",
        "    assert(len(self.data)==len(self.labels))\n",
        "    self.length = len(self.labels)\n",
        "    self.context = context\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "  def __getitem__(self,index):\n",
        "\n",
        "    # TODO\n",
        "    # pad the data with context\n",
        "    # return padded data, label\n",
        "\n",
        "    return NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MaBX9f1H25w"
      },
      "source": [
        "### Load the Data\n",
        "train_data = # Data file name\n",
        "train_label = # Label file name\n",
        "val_data = # Data file name\n",
        "val_label = # Label file name\n",
        "test_data = # Data file name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj-b3rzgTnFw"
      },
      "source": [
        "## Dataloaders\n",
        "\n",
        "# Training dataloader\n",
        "train_data = MLPDataset(x_train, labels_train)\n",
        "train_args = dict(shuffle = True, batch_size = 2, num_workers=4)\n",
        "train_loader = DataLoader(train_data, **train_args)\n",
        "\n",
        "# Validation dataloader\n",
        "val_data = MLPDataset(x_val, labels_val)\n",
        "val_args = dict(shuffle = False, batch_size = 1, num_workers=4)\n",
        "val_loader = DataLoader(val_data, **val_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVJwuYbtTnFw"
      },
      "source": [
        "## Model Architecture definition\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    # define model elements\n",
        "    def __init__(self, size):\n",
        "        super(MLP, self).__init__()\n",
        "        \n",
        "        # Sequential model definition: Input -> Linear -> ReLU -> Linear -> Output\n",
        "        \n",
        "        self.model = nn.Sequential(nn.Linear(size[0], size[1]), nn.ReLU(),\n",
        "                                   nn.Linear(size[1], size[2]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Model forward pass\n",
        "        return NotImplemented"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U865X7LWTnFw"
      },
      "source": [
        "# Model\n",
        "model = MLP([3,128,2])\n",
        "\n",
        "# Define Criterion/ Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define Adam Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Lw80z9KTnFx"
      },
      "source": [
        "# Train the model\n",
        "\n",
        "def train_model(train_loader, model):\n",
        "    training_loss = 0\n",
        "    \n",
        "    # Set model in 'Training mode'\n",
        "    model.train()\n",
        "    \n",
        "    # enumerate mini batches\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        \n",
        "        # clear the gradients\n",
        "        \n",
        "        \n",
        "        # compute the model output\n",
        "        out = \n",
        "        \n",
        "        # calculate loss\n",
        "        loss = \n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update model weights\n",
        "        optimizer.step()\n",
        "\n",
        "        training_loss += loss.item()\n",
        "    training_loss /= len(train_loader)\n",
        "    return training_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73l8p3IWTnFz"
      },
      "source": [
        "# Evaluate the model\n",
        "\n",
        "def evaluate_model(val_loader, model):\n",
        "    \n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    \n",
        "    # Set model in validation mode\n",
        "    model.eval()\n",
        "    \n",
        "    for i, (inputs, targets) in enumerate(val_loader):\n",
        "        \n",
        "        # evaluate the model on the validation set\n",
        "        out = \n",
        "                \n",
        "        # Calculate validation loss\n",
        "        loss = \n",
        "        \n",
        "        # retrieve numpy array\n",
        "        out = \n",
        "        actual = \n",
        "\n",
        "        \n",
        "        # convert to class labels\n",
        "        out = \n",
        "        \n",
        "        # reshape for stacking\n",
        "        actual = \n",
        "        out = \n",
        "        # store\n",
        "        predictions.append(out)\n",
        "        actuals.append(actual)\n",
        "    \n",
        "    predictions, actuals = np.vstack(predictions), np.vstack(actuals)\n",
        "    # Calculate validation accuracy\n",
        "    acc = accuracy_score(actuals, predictions)\n",
        "    return acc, loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOM60IGtTnF0"
      },
      "source": [
        "# Define number of epochs\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # Train\n",
        "    training_loss = train_model(train_loader, model)\n",
        "\n",
        "    # Validation\n",
        "    val_acc, val_loss = evaluate_model(train_loader, model)\n",
        "\n",
        "    # scheduler \n",
        "    \n",
        "    # Print log of accuracy and loss\n",
        "    print(\"Epoch: \"+str(epoch)+\", Training loss: \"+str(training_loss)+\", Validation loss:\"+str(val_loss)+\n",
        "          \", Validation accuracy:\"+str(val_acc*100)+\"%\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}